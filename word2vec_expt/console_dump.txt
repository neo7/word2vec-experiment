Start of Console Dump

########## Start ##############

#### Modify RNNLM library to run on the OSX platform #########
saurabhtiwari:rnnlm]$ vim rnnlmlib.cpp

####### Compiling the RNNLM libraries ####### 
saurabhtiwari:rnnlm]$ g++ -lm -O3 -march=native -Wall -funroll-loops -ffast-math -c rnnlmlib.cpp
clang: warning: -lm: 'linker' input unused

##Compile the RNNLM #########

saurabhtiwari:rnnlm]$ g++ -lm -O3 -march=native -Wall -funroll-loops -ffast-math rnnlm.cpp rnnlmlib.o -o rnnlm

########## Taking the top 12300 lines of Train-pos.txt and adding it to train for train purpose ########
saurabhtiwari:rnnlm]$ head -n 12300 ../aclImdb/train-pos.txt  > train

########## Taking bottom 200 lines of train-pos.txt and adding it to valid for validation purpose ######
saurabhtiwari:rnnlm]$ tail -n 200 ../aclImdb/train-pos.txt  > valid

##### Creating training models in the binary format using RNNLM libraries.####
##### Saved file name train #######
saurabhtiwari:rnnlm]$ ./rnnlm -rnnlm model-pos -train train -valid valid -hidden 50 -direct-order 3 -direct 200 -class 100 -debug 2 -bptt 4 -bptt-block 10 -binary
debug mode: 2
train file: train
valid file: valid
class size: 100
Hidden layer size: 50
Direct connections: 200M
Order of direct connections: 3
BPTT: 4
BPTT block: 10
Model will be saved in binary format
rnnlm file: model-pos
Starting training using file train
Vocab size: 77038
Words in train file: 3302770
Iter:   0	Alpha: 0.100000	   TRAIN entropy: 8.2253    Words/sec: 16091.6   VALID entropy: 7.9094
Iter:   1	Alpha: 0.100000	   TRAIN entropy: 7.5518    Words/sec: 17192.1   VALID entropy: 7.7408
Iter:   2	Alpha: 0.100000	   TRAIN entropy: 7.1893    Words/sec: 17645.8   VALID entropy: 7.6554
Iter:   3	Alpha: 0.100000	   TRAIN entropy: 6.9124    Words/sec: 17415.2   VALID entropy: 7.5954
Iter:   4	Alpha: 0.100000	   TRAIN entropy: 6.6795    Words/sec: 17518.3   VALID entropy: 7.5590
Iter:   5	Alpha: 0.100000	   TRAIN entropy: 6.4797    Words/sec: 16894.9   VALID entropy: 7.5430
Iter:   6	Alpha: 0.050000	   TRAIN entropy: 6.2307    Words/sec: 17179.9   VALID entropy: 7.4039
Iter:   7	Alpha: 0.025000	   TRAIN entropy: 6.1077    Words/sec: 17492.1   VALID entropy: 7.3233
Iter:   8	Alpha: 0.012500	   TRAIN entropy: 6.0491    Words/sec: 17155.8   VALID entropy: 7.2828
Iter:   9	Alpha: 0.006250	   TRAIN entropy: 6.0237    Words/sec: 17588.6   VALID entropy: 7.2623

########## Performing the Operations performed above for the Negative movie reviews #########
########## File names changed accordingly ##############

saurabhtiwari:rnnlm]$ head -n 12300 ../aclImdb/train-neg.txt > train
saurabhtiwari:rnnlm]$ tail -n 200 ../aclImdb/train-neg.txt > valid

########## Creating the training model for negative movie reviews as done above for the positive ones ###########
saurabhtiwari:rnnlm]$ ./rnnlm -rnnlm model-neg -train train -valid valid -hidden 50 -direct-order 3 -direct 200 -class 100 -debug 2 -bptt 4 -bptt-block 10 -binary
debug mode: 2
train file: train
valid file: valid
class size: 100
Hidden layer size: 50
Direct connections: 200M
Order of direct connections: 3
BPTT: 4
BPTT block: 10
Model will be saved in binary format
rnnlm file: model-neg
    1 -340.769682 -351.449554 0.969612
    2 -902.039329 -914.209159 0.986688
    3 -610.323610 -634.907836 0.961279
    4 -246.098784 -256.483059 0.959513
    5 -502.536770 -534.669918 0.939901
    6 -847.496343 -892.382689 0.949701
    7 -848.718062 -870.810887 0.97463
    8 -361.956586 -360.183663 1.00492
    9 -384.280078 -401.829859 0.956325
   10 -2979.567365 -3030.110767 0.98332
   11 -557.063761 -575.635702 0.967737
   12 -306.568628 -325.590587 0.941577
   13 -420.465681 -433.257765 0.970475
   14 -379.644759 -400.176429 0.948693
   15 -215.813610 -219.157757 0.984741
   16 -831.847888 -901.170862 0.923075
   17 -514.929459 -512.099696 1.00553
   18 -2095.736450 -2150.331225 0.974611
   19 -454.152538 -464.846956 0.976994
   20 -301.856294 -302.803958 0.99687
   21 -718.611510 -730.073971 0.9843
   22 -500.555877 -517.017280 0.968161
   23 -522.820613 -532.657699 0.981532
   24 -110.670060 -121.723340 0.909193
   25 -280.751489 -297.641228 0.943255
   26 -344.711254 -365.955774 0.941948
   27 -758.064119 -747.367787 1.01431
   28 -303.998048 -305.434994 0.995295
   29 -453.123467 -491.324045 0.92225
   30 -3143.000018 -3298.379366 0.952892
   31 -1287.956652 -1327.280709 0.970372
   32 -429.431801 -485.076994 0.885286
   33 -2729.443552 -2835.464017 0.962609
   34 -306.696890 -325.616685 0.941895
   35 -691.551430 -711.554976 0.971888
   36 -2252.519521 -2359.807328 0.954535
   37 -975.283556 -1021.956278 0.95433
   38 -295.580695 -324.611132 0.910569
"RNNLM-SCORE" 25000L, 815793C

Starting training using file train
Vocab size: 75661
Words in train file: 3225724
Iter:   0	Alpha: 0.100000	   TRAIN entropy: 8.1276    Words/sec: 16533.3   VALID entropy: 7.8818
Iter:   1	Alpha: 0.100000	   TRAIN entropy: 7.4586    Words/sec: 17144.2   VALID entropy: 7.6929
Iter:   2	Alpha: 0.100000	   TRAIN entropy: 7.1128    Words/sec: 17791.7   VALID entropy: 7.6219
Iter:   3	Alpha: 0.100000	   TRAIN entropy: 6.8525    Words/sec: 16943.7   VALID entropy: 7.5649
Iter:   4	Alpha: 0.100000	   TRAIN entropy: 6.6332    Words/sec: 18310.2   VALID entropy: 7.5409
Iter:   5	Alpha: 0.100000	   TRAIN entropy: 6.4420    Words/sec: 16913.7   VALID entropy: 7.5273
Iter:   6	Alpha: 0.050000	   TRAIN entropy: 6.1884    Words/sec: 16884.9   VALID entropy: 7.4200
Iter:   7	Alpha: 0.025000	   TRAIN entropy: 6.0618    Words/sec: 17634.4   VALID entropy: 7.3697
Iter:   8	Alpha: 0.012500	   TRAIN entropy: 6.0012    Words/sec: 17640.6   VALID entropy: 7.3312
Iter:   9	Alpha: 0.006250	   TRAIN entropy: 5.9746    Words/sec: 19118.0   VALID entropy: 7.3078
Iter:  10	Alpha: 0.003125	   TRAIN entropy: 5.9668    Words/sec: 19714.8   VALID entropy: 7.2945

############### Saving the test samples to test.txt #############
saurabhtiwari:rnnlm]$ cat ../aclImdb/test-pos.txt ../aclImdb/test-neg.txt > test.txt

############## saving the ids into test-id.txt ############
saurabhtiwari:rnnlm]$ awk 'BEGIN{a=0;}{print a " " $0; a++;}' < test.txt > test-id.txt

########## Finding out the positive model score ############# 
saurabhtiwari:rnnlm]$ ./rnnlm -rnnlm model-pos -test test-id.txt -debug 0 -nbest > model-pos-score
######### Performing the above operation for negative models #############
saurabhtiwari:rnnlm]$ ./rnnlm -rnnlm model-neg -test test-id.txt -debug 0 -nbest > model-neg-score

############# Saving all scores into RNNLM score ################
saurabhtiwari:rnnlm]$ paste model-pos-score model-neg-score | awk '{print $1 " " $2 " " $1/$2;}' > ../RNNLM-SCORE

########### Move one level out from the directory ############
saurabhtiwari:rnnlm]$ cd ..

############### Moving inside word2vec libraries ##############
saurabhtiwari:word2vec_expt]$ cd word2vec-read-only/

################ Compiling Word2vec ###################
saurabhtiwari:word2vec-read-only]$ gcc word2vec.c -o word2vec -lm -pthread -O3 -march=native -funroll-loops

################ Creating the word2vec vectors using word2vec libraries##############
################ Saving the vectors in vectors.txt ###########################

saurabhtiwari:word2vec-read-only]$ time ./word2vec -train ../aclImdb/alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1
———Vector creation started————
Starting training using file ../aclImdb/alldata-id.txt
Vocab size: 356414
Words in train file: 26499251
Alpha: 0.000002  Progress: 100.02%  Words/thread/sec: 63.74k
real	38m20.783s
user	138m25.973s
sys	0m38.613s
————— Vector creation ends —————

############Moving out sentence vectors to the file called sentence_vectors.txt #############
saurabhtiwari:word2vec-read-only]$ grep '_\*' vectors.txt > sentence_vectors.txt

#######Download lib linear 2.1 from the internet and place it inside the directory. ########

######### Decompress Liblinear library ###########
saurabhtiwari:word2vec-read-only]$ unzip liblinear-2.1.zip
Archive:  liblinear-2.1.zip
   creating: liblinear-2.1/
   creating: liblinear-2.1/blas/
  inflating: liblinear-2.1/blas/blasp.h
  inflating: liblinear-2.1/blas/dnrm2.c
  inflating: liblinear-2.1/blas/Makefile
  inflating: liblinear-2.1/blas/ddot.c
  inflating: liblinear-2.1/blas/dscal.c
  inflating: liblinear-2.1/blas/blas.h
  inflating: liblinear-2.1/blas/daxpy.c
  inflating: liblinear-2.1/COPYRIGHT
  inflating: liblinear-2.1/linear.h
  inflating: liblinear-2.1/README
  inflating: liblinear-2.1/Makefile.win
  inflating: liblinear-2.1/train.c
  inflating: liblinear-2.1/heart_scale
  inflating: liblinear-2.1/tron.cpp
  inflating: liblinear-2.1/Makefile
  inflating: liblinear-2.1/linear.def
   creating: liblinear-2.1/windows/
  inflating: liblinear-2.1/windows/train.exe
  inflating: liblinear-2.1/windows/predict.mexw64
  inflating: liblinear-2.1/windows/train.mexw64
  inflating: liblinear-2.1/windows/predict.exe
  inflating: liblinear-2.1/windows/libsvmwrite.mexw64
  inflating: liblinear-2.1/windows/libsvmread.mexw64
  inflating: liblinear-2.1/windows/liblinear.dll
  inflating: liblinear-2.1/linear.cpp
  inflating: liblinear-2.1/predict.c
  inflating: liblinear-2.1/tron.h
   creating: liblinear-2.1/python/
  inflating: liblinear-2.1/python/README
  inflating: liblinear-2.1/python/liblinearutil.py
  inflating: liblinear-2.1/python/Makefile
  inflating: liblinear-2.1/python/liblinear.py
   creating: liblinear-2.1/matlab/
  inflating: liblinear-2.1/matlab/README
  inflating: liblinear-2.1/matlab/linear_model_matlab.h
  inflating: liblinear-2.1/matlab/train.c
  inflating: liblinear-2.1/matlab/Makefile
  inflating: liblinear-2.1/matlab/make.m
  inflating: liblinear-2.1/matlab/libsvmread.c
  inflating: liblinear-2.1/matlab/libsvmwrite.c
  inflating: liblinear-2.1/matlab/predict.c
  inflating: liblinear-2.1/matlab/linear_model_matlab.c


####### Move inside lib linear 2.1 folder created. ##########
saurabhtiwari:word2vec-read-only]$ cd liblinear-2.1

############# Build the project #################
saurabhtiwari:liblinear-2.1]$ make
c++ -Wall -Wconversion -O3 -fPIC -c -o tron.o tron.cpp
tron.cpp:98:35: warning: implicit conversion changes signedness: 'int' to 'unsigned long' [-Wsign-conversion]
                memcpy(w_new, w, sizeof(double)*n);
                                               ~^
tron.cpp:134:36: warning: implicit conversion changes signedness: 'int' to 'unsigned long' [-Wsign-conversion]
                        memcpy(w, w_new, sizeof(double)*n);
                                                       ~^
2 warnings generated.
c++ -Wall -Wconversion -O3 -fPIC -c -o linear.o linear.cpp
linear.cpp:510:11: warning: implicit conversion changes signedness: 'int' to 'size_t' (aka 'unsigned long') [-Wsign-conversion]
        qsort(D, active_i, sizeof(double), compare_double);
        ~~~~~    ^~~~~~~~
linear.cpp:2014:33: warning: implicit conversion changes signedness: 'int' to 'unsigned long' [-Wsign-conversion]
        x_space = new feature_node[nnz+n];
—
—
—
—
—
Output trimmed for saving space
-
-
-
-
                                      ~^
35 warnings generated.
make -C blas OPTFLAGS='-Wall -Wconversion -O3 -fPIC' CC='cc';
cc -Wall -Wconversion -O3 -fPIC  -c dnrm2.c
cc -Wall -Wconversion -O3 -fPIC  -c daxpy.c
cc -Wall -Wconversion -O3 -fPIC  -c ddot.c
cc -Wall -Wconversion -O3 -fPIC  -c dscal.c
ar rcv blas.a dnrm2.o daxpy.o ddot.o dscal.o
a - dnrm2.o
a - daxpy.o
a - ddot.o
a - dscal.o
ranlib  blas.a
c++ -Wall -Wconversion -O3 -fPIC -o train train.c tron.o linear.o blas/blas.a
clang: warning: treating 'c' input as 'c++' when in C++ mode, this behavior is deprecated
train.c:77:32: warning: implicit conversion changes signedness: 'int' to 'size_t' (aka 'unsigned long') [-Wsign-conversion]
                line = (char *) realloc(line,max_line_len);
                                ~~~~~~~      ^~~~~~~~~~~~
train.c:163:39: warning: implicit conversion changes signedness: 'int' to 'unsigned long' [-Wsign-conversion]
        double *target = Malloc(double, prob.l);
                         ~~~~~~~~~~~~~~~~~~~~^~
train.c:8:40: note: expanded from macro 'Malloc'
#define Malloc(type,n) (type *)malloc((n)*sizeof(type))
                                       ^

6 warnings generated.




####### Compilation ends here ############

######### Move one level out of the directory ##########
saurabhtiwari:liblinear-2.1]$ cd ..

######### Grabbing 25000 lines of Sentence vector file and dumping it to train.txt ###########
saurabhtiwari:word2vec-read-only]$ head -n 25000 sentence_vectors.txt | awk 'BEGIN{a=0;}{if (a<12500) printf "1 "; else printf "-1 "; for (b=1; b<NF; b++) printf b ":" $(b+1) " "; print ""; a++;}' > train.txt

######### Grabbing 50000 lines of Sentence vector file and dumping it to test.txt ###########
saurabhtiwari:word2vec-read-only]$ head -n 50000 sentence_vectors.txt | tail -n 25000 | awk 'BEGIN{a=0;}{if (a<12500) printf "1 "; else printf "-1 "; for (b=1; b<NF; b++) printf b ":" $(b+1) " "; print ""; a++;}' > test.txt

######### Using lib linear library to train data and saving the data to model.logreg ############# 
saurabhtiwari:word2vec-read-only]$ ./liblinear-2.1/train -s 0 train.txt model.logreg
iter  1 act 2.805e+02 pre 2.772e+02 delta 8.752e-01 f 1.733e+04 |g| 7.425e+02 CG   5
iter  2 act 2.312e+00 pre 2.309e+00 delta 8.752e-01 f 1.705e+04 |g| 5.629e+01 CG   7
iter  3 act 1.841e-02 pre 1.841e-02 delta 8.752e-01 f 1.705e+04 |g| 5.180e+00 CG   7

######### Predicting the output from test.txt from model.logreg and sending data to out.logreg ##########
saurabhtiwari:word2vec-read-only]$ ./liblinear-2.1/predict -b 1 test.txt model.logreg out.logreg
Accuracy = 51.948% (12987/25000)

######### pasting last 25000 lines to Sentence-vector.logreg#######################
saurabhtiwari:word2vec-read-only]$ tail -n 25000 out.logreg > ../SENTENCE-VECTOR.LOGREG

########## moving one level up from the current directory ###############
saurabhtiwari:word2vec-read-only]$ cd ..

##################### Calculating Accuracy ####################
saurabhtiwari:word2vec_expt]$ cat RNNLM-SCORE | awk ' \
 BEGIN{cn=0; corr=0;} \
 { \
   if ($3<1) if (cn<12500) corr++; \
   if ($3>1) if (cn>=12500) corr++; \
   cn++; \
 } \
 END{print "RNNLM accuracy: " corr/cn*100 "%";}'
RNNLM accuracy: 86.536%

########## Calculating Sentence vector and logistic Regression accuracy #############
saurabhtiwari:word2vec_expt]$ cat SENTENCE-VECTOR.LOGREG | awk ' \
 BEGIN{cn=0; corr=0;} \
 { \
   if ($2>0.5) if (cn<12500) corr++; \
   if ($2<0.5) if (cn>=12500) corr++; \
   cn++; \
 } \
 END{print "Sentence vector + logistic regression accuracy: " corr/cn*100 "%";}'
Sentence vector + logistic regression accuracy: 51.948%


########### Finding the final accuracy ############## 
#### From RNNLM and Sentence vector ###########
saurabhtiwari:word2vec_expt]$ paste RNNLM-SCORE SENTENCE-VECTOR.LOGREG | awk ' \
 BEGIN{cn=0; corr=0;} \
 { \
   if (($3-1)*7+(0.5-$5)<0) if (cn<12500) corr++; \
   if (($3-1)*7+(0.5-$5)>0) if (cn>=12500) corr++; \
   cn++; \
 } \
 END{print "FINAL accuracy: " corr/cn*100 "%";}'
FINAL accuracy: 85.364%


—————— Download support files for simulating the experiment ———— 

https://s3.amazonaws.com/modelfilernnlm/projectcs585models/model-pos
https://s3.amazonaws.com/modelfilernnlm/projectcs585models/model-neg
